\documentclass{article}

\usepackage{graphicx} % Required for inserting images
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} \usepackage{amsmath}
\usepackage{amsthm} %proof environment
\usepackage{amsthm} %proof environment
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumitem} %nice lists
\usepackage{verbatim} %useful for something 
\usepackage{xcolor}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{blindtext} % I have no idea what this is 
\usepackage{caption}  % need this for unnumbered captions/figures
\usepackage{natbib}
\usepackage{appendix}
\usepackage{tikz}
\usepackage{hyperref}


\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\titleformat{\section}{\bfseries\Large}{Problem \thesection:}{5pt}{}

\begin{document}

\title{AM 216 - Stochastic Differential Equations: Assignment }
\author{Dante Buhl}


\newcommand{\wrms}{w_{\text{rms}}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\bmp}[1]{\begin{minipage}{#1\textwidth}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Bino}{\text{Bino}}
\newcommand{\Norm}{\mathcal{N}}
\newcommand{\erf}{\text{erf}}
%\newcommand{\K}{\bs{\mathrm{K}}}
\newcommand{\m}{\bs{\mu}_*}
\newcommand{\s}{\bs{\Sigma}_*}
\newcommand{\dt}{\Delta t}
\newcommand{\dx}{\Delta x}
\newcommand{\tr}[1]{\text{Tr}(#1)}
\newcommand{\Tr}[1]{\text{Tr}(#1)}
\newcommand{\Div}{\nabla \cdot}
\renewcommand{\div}{\nabla \cdot}
\newcommand{\Curl}{\nabla \times}
\newcommand{\Grad}{\nabla}
\newcommand{\grad}{\nabla}
\newcommand{\grads}{\nabla_s}
\newcommand{\gradf}{\nabla_f}
\newcommand{\xs}{x_s}
\newcommand{\x}{\bs{x}}
\newcommand{\xf}{x_f}
\newcommand{\ts}{t_s}
\newcommand{\tf}{t_f}
\newcommand{\pt}{\partial t}
\newcommand{\pz}{\partial z}
\newcommand{\uvec}{\bs{u}}
\newcommand{\bvec}{\bs{B}}
\newcommand{\nvec}{\hat{\bs{n}}}
\newcommand{\tu}{\tilde{\uvec}}
\newcommand{\B}{\bs{B}}
\newcommand{\A}{\bs{A}}
\newcommand{\jvec}{\bs{j}}
\newcommand{\F}{\bs{F}}
\newcommand{\T}{\tilde{T}}
\newcommand{\ez}{\bs{e}_z}
\newcommand{\ex}{\bs{e}_x}
\newcommand{\ey}{\bs{e}_y}
\newcommand{\eo}{\bs{e}_{\bs{\Omega}}}
\newcommand{\ppt}[1]{\frac{\partial #1}{\partial t}}
\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pptwo}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\ddtwo}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\DDt}[1]{\frac{D #1}{D t}}
\newcommand{\ppts}[1]{\frac{\partial #1}{\partial t_s}}
\newcommand{\pptf}[1]{\frac{\partial #1}{\partial t_f}}
\newcommand{\ppz}[1]{\frac{\partial #1}{\partial z}}
\newcommand{\ddz}[1]{\frac{d #1}{d z}}
\newcommand{\ppzetas}[1]{\frac{\partial^2 #1}{\partial \zeta^2}}
\newcommand{\ppzs}[1]{\frac{\partial #1}{\partial z_s}}
\newcommand{\ppzf}[1]{\frac{\partial #1}{\partial z_f}}
\newcommand{\ppx}[1]{\frac{\partial #1}{\partial x}}
\newcommand{\ddx}[1]{\frac{d #1}{d x}}
\newcommand{\ppxi}[1]{\frac{\partial #1}{\partial x_i}}
\newcommand{\ppxj}[1]{\frac{\partial #1}{\partial x_j}}
\newcommand{\ppy}[1]{\frac{\partial #1}{\partial y}}
\newcommand{\ppzeta}[1]{\frac{\partial #1}{\partial \zeta}}
\renewcommand{\k}{\bs{k}}
\newcommand{\real}[1]{\text{Re}\left[#1\right]}


\maketitle 
% This line removes the automatic indentation on new paragraphs
\setlength{\parindent}{0pt}

\section{Time Reversability of Brownian Bridge}
    \begin{proof}
        We begin by defining $W_j$. We have, 
        \begin{gather*}
            W_j = S_j = \sum_{k=0}^{j-1} \Delta W_k, \quad \Delta W_k \sim
            \sqrt{\Delta t}X_k, \quad X_k \sim N(0,1)
        \end{gather*}
        Thus we begin, 
        \begin{align*}
            E(W_j | W_n = 0) &= E(B_j)
            \\
            &= E\left(W_j - \frac{t_j}{T}W_n\right)
            \\ 
            &= \sum_{k=0}^{j-1} E(\Delta W_k) -
            \frac{t_j}{T}\sum_{k=0}{n-1}E(\Delta W_k)
            \\
            &= 0
        \end{align*}
        Next we look at the covariance between the two sets, 
        \begin{align*}
            \Cov(W_i, W_j | W_n = 0) &= \Cov(B_i, B_j) 
            \\
            &= E(B_iB_j) - E(B_i)E(B_j)
            \\
            &= E\left[\left(S_i - \frac{t_i}{T}S_n\right)\left(S_j -
            \frac{t_j}{T}S_n\right)\right]
            \\
            &= E(S_iS_j) - \frac{t_i}{T}E(S_jS_n) - \frac{t_j}{T}E(S_iS_n) +
            \frac{t_it_j}{T^2}E(S_n^2)
        \end{align*}
        We now look more specifically at the components of the sums $S_i, S_j,
        \text{ and } S_n$.  Take for example, $j > i$, we have then, 
        \begin{gather*}
            S_i = a, \quad S_j = a + b, \quad S_n = a + b + c
            \\
            a = \sum_{k=0}^{i-1} \Delta W_k, \quad b = \sum_{k=i}^{j-1} \Delta
            W_k, \quad c = \sum_{k=j}^{n-1}\Delta W_k
        \end{gather*}
        Notice that each component $a,b,c$ is independent of each other,
        $E(a) = E(b) = E(c) = 0$, and $E(a^2) = t_i$, $E((a+b)^2) = t_j$, and
        $E((a+b+c)^2) = T$. Thus, 
        \begin{align*}
            \Cov(W_i, W_j | W_n = 0) &= E(a^2 + ab) - \frac{t_i}{T}E\left((a + b)^2
            + (a + b)c\right) - \frac{t_j}{T}E(a^2 + a(b+c)) +
            \frac{t_it_j}{T^2}E\left((a+b+c)^2\right)
            \\
            &= E(a^2) - \frac{t_i}{T}E\left((a + b)^2\right) - 
            \frac{t_j}{T}E(a^2) +
            \frac{t_it_j}{T^2}E\left((a+b+c)^2\right)
            \\
            &= t_i - 2\frac{t_it_j}{T} + \frac{t_it_j}{T}
            \\
            &= t_i\left(1 - \frac{t_j}{T}\right)
        \end{align*}
        Now, in order to show time reversability we demonstrate that $E(W_{n-j}
        | W_n = 0) = E(W_{j} | W_n = 0)$ and $\Cov(W_i,W_j|W_n = 0) =
        \Cov(W_{n-i},W_{n-j},W_n=0)$. The case for the expectation is trivial as
        the expectation is zero. For the covariance, we have,
        \begin{align*}
           \Cov(W_{n-i},W_{n-j}|W_n=0) &= \Cov(B_{n-i},B_{n-j}) 
           \\
           &= E(S_{n-i}S_{n-j}) - \frac{T-t_i}{T}E(S_{n-j}S_n) 
           \\
           &-
           \frac{T - t_j}{T}E(S_{n-i}S_n) +
            \frac{(T-t_i)(T-t_j)}{T^2}E(S_n^2)
        \end{align*}
        Since we had previously that $j > i$, we now have $n-i > n-j$. Thus, we 
        have 
        \begin{gather*}
            S_{n-j} = d, \quad S_{n-i} = d + e, \quad S_n = d + e + f
            \\
            d = \sum_{k=0}^{n-j-1} \Delta W_k, \quad b = \sum_{k=n-j}^{n-i-1}   
            \Delta W_k, \quad c = \sum_{k=n-i}^{n-1}\Delta W_k
            \\
            E(d) = E(e) = E(f) = 0, \quad E(d^2) = T - t_j, \quad E((d+e)^2) = T
            - t_i, \quad E((d + e + f)^2) = T
        \end{gather*}
        Therefore, we now can show,
        \begin{align*}
            \Cov(W_{n-i},W_{n-j}|W_n=0) &= (T-t_j) -
            \frac{(T-t_i)(T-t_j)}{T} -
           \frac{(T - t_j)(T-t_i)}{T} +
            \frac{(T-t_i)(T-t_j)}{T}
            \\
            &= (T-t_j)\frac{(1 - T + t_i)}{T}
            \\
            &= t_i\left(1 - \frac{t_j}{T}\right) = \Cov(W_i, W_j | W_n = 0)
        \end{align*}
        This concludes the proof that the brownian bridge is reversable in time,
        i.e. that moving backwards is distributed identically to moving
        forwards. This is attributable to the fact that there is no correlation
        between the direction of movement $\Delta W$ to the direction of time
        $\Delta t$. Only the amplitudes of the adjustments are correlated. 
    \end{proof}

\section{Convergence in Probability}
    We have, 
    \begin{align*}
        \lim_{n\to\infty}\Var(Q_n(w)) &= 0
        \\
        &= \lim_{n\to\infty}\left(E(Q_n^2) - E^2(Q_n)\right)
        \\
        \lim_{n\to\infty} E(Q_n^2) &= q^2
        \\
        \lim_{n\to\infty}E(|Q_n - q|^2) &\ge
        \varepsilon^2\lim_{n\to\infty}Pr(|Q_n - q| \ge \varepsilon)
        \\
        \lim_{n\to\infty}Pr(|Q_n - q| \ge \varepsilon) \le 0
    \end{align*}
    thus we can state that $\{Q_n\}$ converges to $q$ in probability as $n \to
    \infty$. 

\section{Gaussians}
    \begin{enumerate}[label=\roman*)]
        \item 
            \begin{align*}
                I_2 &= \int_0^T\cos\left(n\pi\frac{t}{T}\right)dW(t)
                \\
                &= \int_0^T\cos\left(n\pi\frac{t}{T}\right)\sqrt{dt}N(0,1)
                \\
                &= N\left(0, \int_0^T\cos^2\left(n\pi\frac{t}{T}\right)dt\right)
                \\
                &= N\left(0, \frac{1}{2}\int_0^T1 + \cos\left(2n\pi\frac{t}{T}\right)dt\right)
                \\
                &= N\left(0,
                \frac{T}{2}+\frac{T}{2n\pi}\sin\left(2n\pi\frac{t}{T}\right)\Big|_0^T\right)
                \\
                &= N\left(0, \frac{T}{2}\right)
            \end{align*}
            Thus we have, $E(I_2) = 0$ and $\Var(I_2) = T/2$
        \item 
            \begin{align*}
                F_n &=
                \frac{2}{T}\int_0^T\sin\left(n\pi\frac{t}{T}\right)\left(W(t) -
                \frac{t}{T}W(T)\right)dt
                \\
                &=
                \frac{2}{T}\left(\frac{T}{n\pi}\cos\left(n\pi\frac{t}{T}\right)
                \left(W(t) - \frac{t}{T}W(T)\right)\Big|_0^T - 
                \frac{T}{n\pi}\int_0^T\cos\left(n\pi\frac{t}{T}\right)
                \left(dW(T) - \frac{dt}{T}W(T)\right)\right)
                \\
                &= \frac{2}{n\pi}\int_0^T\cos\left(n\pi\frac{t}{T}\right)
                \left(dW(T) - \frac{dt}{T}W(T)\right)
                \frac{2}{n\pi}\int_0^T
                \\
                &= \frac{2}{n\pi}\int_0^T\left(\cos\left(n\pi\frac{t}{T}\right)
                - \frac{1}{T}\int_0^T\cos\left(n\pi\frac{s}{T}\right)ds\right)dW
                \\
                &= \frac{2}{n\pi}\int_0^T\cos\left(n\pi\frac{t}{T}\right)dW
                \\
                &\sim
                N\left(0,\frac{4}{n^2\pi^2}\int_0^T
                \cos^2\left(n\pi\frac{t}{T}\right)dt\right)
                \\
                &\sim N\left(0,\frac{2T}{n^2\pi^2}\right)
            \end{align*} 
            Thus we have, $E(F_n) = 0$ and $\Var(F_n) = 2T/(n^2\pi^2)$
    \end{enumerate}

\section{Variance of the sums of products of functions of independent
variables lol}
    \begin{proof}
        We begin by examining $\Var(\cdot)$. We have, 
        \begin{align*}
            \Var\left(\sum_{j=0}^{n-1} g(Y_j)f(X_j)\right) &= E(\star^2) -
            E^2(\star), \quad \star = \sum_{j=0}^{n-1} g(Y_j)f(X_j)
            \\
            E(\star) &= \sum_{j=0}^{n-1}E\left(g(Y_j)f(X_j)\right)
            \\
            &= \sum_{j=0}^{n-1}E(g(Y_j))E(f(X_j)) = 0
            \\
            \star^2 &= \sum_{j=0}^{n-1} g^2(Y_j)f^2(X_j) - 2\sum_{i=0,i\ne
            j}^{n-1} g(Y_j)f(X_j)g(Y_i)f(X_i)
            \\
            E(\star^2) &= \sum_{j=0}^{n-1} E\left(g^2(Y_j)f^2(X_j)\right) -
            2\sum_{j=0}^{n-1}\sum_{i=0,i\ne j}^{n-1}
            E\left(g(Y_j)f(X_j)g(Y_i)f(X_i)\right)
            \\
            &= \sum\left(\triangleleft_j\right) -2 \sum\sum\left(\triangleright_{i,j}\right)
        \end{align*}
        Now consider the case for $\triangleright_{i,j}$ where $j>i$. We have that
        $X_j$ is independent of $X_i$, $Y_j$ and $Y_i$ as givens for this
        problem. Thus we have, 
        \begin{align*}
            \triangleright_{i,j} &= E\left(g(Y_j)f(X_j)g(Y_i)f(X_i)\right)
            \\
            &= E(f(X_j))E\left(g(Y_j)g(Y_i)f(X_i)\right) = 0 
            \\
            \triangleleft_j &= E\left(g^2(Y_j)f^2(X_j)\right)
            \\
            &= E\left(g^2(Y_j)\right)E\left(f^2(X_j)\right)
            \\
            \Var(\star) &= \sum_{j=0}^{n-1} \triangleleft_j
            \\
            \Var\left(\sum_{j=0}^{n-1} g(Y_j)f(X_j)\right) &=
            \sum_{j=0}^{n-1} E\left(g^2(Y_j)\right)E\left(f^2(X_j)\right)
        \end{align*}
        The same result holds for $i > j$, as instead of $X_j$ being independent
        of $X_i, Y_i, Y_j$ it is instead $X_i$ which is independent of $X_j,
        Y_i, Y_j$ and the same arguement holds. 
    \end{proof}


\section{Ito's Lemma, again}
    \begin{proof}
        We begin by determining the independence of $W_j$ and $\Delta W_j$. We
        have, 
        \begin{align*}
            W_j &= W_0 + \sum_{i=0}^{j-1} \Delta W_i, \quad W_i \sim N(0,\Delta t)
        \end{align*}
        And so $\Delta W_j$ is completely independent of $W_j$ as it is not
        contained inside the sum which comprises $W_j$. 
        We have next to look at $E(Q_k - f_k)$. 
        \begin{align*}
            E(Q_k - f_k) &= E\left(\sum_{j=0}^{k-1} W_j^2\left(\Delta W_j^2 - \Delta
            t\right)\right)
            \\
            &= \sum_{j=0}^{k-1} E\left(W_j^2\left(\Delta W_j^2 - \Delta
            t\right)\right)
            \\
            &= \sum_{j=0}^{k-1} E(W_j^2)E\left(\Delta W_j^2 - \Delta
            t\right)
            \\
            &= \sum_{j=0}^{k-1} E(W_j^2)\left[\Delta t - \Delta t\right]
            \\
            &= 0
        \end{align*}
        Where the expectation of the products in line 3 is seperabla as we have
        shown independence. Next we look at the variance, we have
        \begin{align*}
            \Var(Q_k - f_k) &= E\left(Q_k-f_k)\right)^2 - E^2(Q_k-f_k)
            \\
            &= E\left(Q_k-f_k)\right)^2
            \\
            &= E\left(\sum_{j=0}^{k-1}W_j^4(\Delta W_j^2 - \Delta t)^2 -
            2\sum_{i=0,i\ne j}^{k-1}W_j^2W_i^2(\Delta W_j^2 - \Delta t)(\Delta
            W_i^2 - \Delta t)\right)
            \\
            &= \sum_{j=0}^{k-1}E\left(W_j^4(\Delta W_j^2 - \Delta t)^2\right) -
            2\sum_{j=0}^{k-1}\sum_{i=0,i\ne j}^{k-1}E\left(W_j^2W_i^2(\Delta W_j^2 - \Delta t)(\Delta
            W_i^2 - \Delta t)\right)
            \\
            &= \left(1\right) + \left(2\right)
        \end{align*}
        Here, we split this calculation into two parts, $(1)$ and $(2)$. Let us
        first look at $(2)$. Notice that while $T_j = W_j^2(\Delta W_j^2 - \Delta t)$
        is comprised of independently distributed products, we do not have that
        $T_i$ is independent of $T_j$. That is, if $j > i$ we have that $W_j$ is
        conditionally dependent on $W_i$ and that $\Delta W_i$ is in the sum
        which comprises $W_j$. We do still have, however, that $\Delta W_j^2$ is
        independent of $W_j$ and $T_i$. Thus, we can write, 
        \begin{align*}
            (2) &= -2\sum_{j=0}^{k-1}\sum_{i=0,i\ne j}^{k-1} E\left((\Delta W_j^2
            - \Delta t)W_j^2 T_i\right)
            \\
            &= -2\sum_{j=0}^{k-1}\sum_{i=0,i\ne j}^{k-1} E(\Delta W_j^2
            - \Delta t)E(W_j^2 T_i)
            \\
            &= 0
        \end{align*}
        This also holds for the case where $i > j$ and so this is true for all
        terms in the sum. Finally, we look at $(1)$. We have, 
        \begin{align*}
            (1) &= \sum_{j=0}^{k-1} E(W_j^4)E(\Delta W_j^2 - \Delta t)^2
            \\
            &= \sum_{j=0}^{k-1} t_j^2E(X_j^4)E(\Delta W_j^4 + \Delta t^2 -
            2\Delta t\Delta W_j^2)
            \\
            &= \sum_{j=0}^{k-1} t_j^2(3)\left(3\Delta t^2 + \Delta t^2 - 2\Delta
            t^2\right)
            \\
            &= \sum_{j=0}^{k-1} 6t_j^2\Delta t^2
        \end{align*}
    \end{proof}
\section{PSD of Ornstein-Uhlenbeck process}
    \begin{align*}
        F\left[e^{-\beta|t|}\right] &\equiv \int_{-\infty}^{\infty} e^{-2\pi\xi
        t}e^{-\beta|t|}dt 
        \\
        &= \int_{-\infty}^{\infty} e^{-2\pi\xi
        t -\beta|t|}dt 
        \\
        &= \int_{-\infty}^{0} e^{(-2\pi\xi + \beta 
        )t }dt + \int_{0}^{\infty} e^{-(2\pi\xi +
        \beta) t}dt
        \\
        &= \frac{1}{-2\pi\xi + \beta}e^{(-2\pi\xi + \beta)t}\Big|_{-\infty}^0 - 
        \frac{1}{2\pi\xi + \beta}e^{-(2\pi\xi + \beta)t}\Big|_0^{\infty}
        \\
        &= \frac{1}{-2\pi\xi + \beta} + \frac{1}{2\pi\xi + \beta}
        \\
        &= \frac{2\beta}{\beta^2 + 4\pi^2\xi^2}
    \end{align*}

\section{Optional: Paley Wiener represation of Wiener process}

\section{Optional: Paley Wiener represation of Wiener process Continued}

\end{document}
