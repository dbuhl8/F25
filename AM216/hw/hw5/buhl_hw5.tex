\documentclass{article}

\usepackage{graphicx} % Required for inserting images
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} \usepackage{amsmath}
\usepackage{amsthm} %proof environment
\usepackage{amsthm} %proof environment
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumitem} %nice lists
\usepackage{verbatim} %useful for something 
\usepackage{xcolor}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{blindtext} % I have no idea what this is 
\usepackage{caption}  % need this for unnumbered captions/figures
\usepackage{natbib}
\usepackage{appendix}
\usepackage{tikz}
\usepackage{hyperref}


\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\titleformat{\section}{\bfseries\Large}{Problem \thesection:}{5pt}{}

\begin{document}

\title{AM 216 - Stochastic Differential Equations: Assignment }
\author{Dante Buhl}


\newcommand{\wrms}{w_{\text{rms}}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\bmp}[1]{\begin{minipage}{#1\textwidth}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Bino}{\text{Bino}}
\newcommand{\Norm}{\mathcal{N}}
\newcommand{\erf}{\text{erf}}
%\newcommand{\K}{\bs{\mathrm{K}}}
\newcommand{\m}{\bs{\mu}_*}
\newcommand{\s}{\bs{\Sigma}_*}
\newcommand{\dt}{\Delta t}
\newcommand{\dx}{\Delta x}
\newcommand{\tr}[1]{\text{Tr}(#1)}
\newcommand{\Tr}[1]{\text{Tr}(#1)}
\newcommand{\Div}{\nabla \cdot}
\renewcommand{\div}{\nabla \cdot}
\newcommand{\Curl}{\nabla \times}
\newcommand{\Grad}{\nabla}
\newcommand{\grad}{\nabla}
\newcommand{\grads}{\nabla_s}
\newcommand{\gradf}{\nabla_f}
\newcommand{\xs}{x_s}
\newcommand{\x}{\bs{x}}
\newcommand{\xf}{x_f}
\newcommand{\ts}{t_s}
\newcommand{\tf}{t_f}
\newcommand{\pt}{\partial t}
\newcommand{\pz}{\partial z}
\newcommand{\uvec}{\bs{u}}
\newcommand{\bvec}{\bs{B}}
\newcommand{\nvec}{\hat{\bs{n}}}
\newcommand{\tu}{\tilde{\uvec}}
\newcommand{\B}{\bs{B}}
\newcommand{\A}{\bs{A}}
\newcommand{\jvec}{\bs{j}}
\newcommand{\F}{\bs{F}}
\newcommand{\T}{\tilde{T}}
\newcommand{\ez}{\bs{e}_z}
\newcommand{\ex}{\bs{e}_x}
\newcommand{\ey}{\bs{e}_y}
\newcommand{\eo}{\bs{e}_{\bs{\Omega}}}
\newcommand{\ppt}[1]{\frac{\partial #1}{\partial t}}
\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pptwo}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\ddtwo}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\DDt}[1]{\frac{D #1}{D t}}
\newcommand{\ppts}[1]{\frac{\partial #1}{\partial t_s}}
\newcommand{\pptf}[1]{\frac{\partial #1}{\partial t_f}}
\newcommand{\ppz}[1]{\frac{\partial #1}{\partial z}}
\newcommand{\ddz}[1]{\frac{d #1}{d z}}
\newcommand{\ppzetas}[1]{\frac{\partial^2 #1}{\partial \zeta^2}}
\newcommand{\ppzs}[1]{\frac{\partial #1}{\partial z_s}}
\newcommand{\ppzf}[1]{\frac{\partial #1}{\partial z_f}}
\newcommand{\ppx}[1]{\frac{\partial #1}{\partial x}}
\newcommand{\ddx}[1]{\frac{d #1}{d x}}
\newcommand{\ppxi}[1]{\frac{\partial #1}{\partial x_i}}
\newcommand{\ppxj}[1]{\frac{\partial #1}{\partial x_j}}
\newcommand{\ppy}[1]{\frac{\partial #1}{\partial y}}
\newcommand{\ppzeta}[1]{\frac{\partial #1}{\partial \zeta}}
\renewcommand{\k}{\bs{k}}
\newcommand{\real}[1]{\text{Re}\left[#1\right]}


\maketitle 
% This line removes the automatic indentation on new paragraphs
\setlength{\parindent}{0pt}

\section{Time inversion property of the Wiener process}
    
    \begin{proof}   
        We begin by looking at the covariance of the two sets. We have, 
        \begin{align*}
            \Cov(W_i, W_j) &= E(W_iW_j) - E(W_i)E(W_j), \quad j > i, \quad W_j =
            a + b, \quad W_i = a
            \\
            &= E(a^2 + ab)
            \\
            &= E(a^2)
            \\
            &= E(W_i^2)
            \\
            &= t_i
        \end{align*}
        This holds for $j > i$ and if $i > j$ then we have $\Cov(W_i, W_j) =
        t_j$. Thus we have, $\Cov(W_i, W_j) = \min(t_i, t_j)$. Next we look at
        \begin{align*}
            \Cov(B_i, B_j) &= t_it_j\left(E(W_{1/i}W_{1/j}) -
            E(W_{1/i})E(W_{1/j})\right)
            \\
            &= t_it_jE(W_{1/\max(t_i,t_j)}^2)
            \\
            &= t_it_j\frac{1}{\max(t_i,t_j)}
            \\
            &= \min(t_i,t_j)
        \end{align*}
        As the problem hint states, each of these RV is clearly gaussian, and
        since they have the same covariance, we have that they are iid. That is, 
        \begin{align*}
            \{B_i\} \sim \{W_i\}
        \end{align*}
    \end{proof}

\section{Integrating the Wiener process}
    \begin{enumerate}[label=\roman*)]
        \item We have $W(t) = \int_0^t dW(s)$
            \begin{proof}
                We begin, 
                \begin{align*}
                    \int_0^T\int_0^s dW(s)dt &= \int_0^T\int_s^T dtdW(s)
                    \\
                    &= \int_0^T (T - s)dW(s)
                    \\
                    &\sim N\left(0, \int_0^T (T-s)^2dt\right)
                    \\
                    &\sim N\left(0, T^3 - T^3 + \frac{T^3}{3}\right)
                    \\
                    &\sim N\left(0, \frac{T^3}{3}\right)
                \end{align*}
            \end{proof}
        \item We repeat but with a new integrand
            \begin{proof}
                \begin{align*}
                    \int_0^T\int_0^s tdW(s)dt &= \int_0^T\int_s^T tdtdW(s)
                    \\
                    &= \frac{1}{2}\int_0^T (T^2 - s^2)dW(s)
                    \\
                    &\sim N\left(0, \frac{1}{4}\int_0^T (T^2-s^2)^2dt\right)
                    \\
                    &\sim N\left(0, \frac{1}{4}\left(T^5 - \frac{2}{3}T^5 +
                    \frac{1}{5}T^5\right)\right)
                    \\
                    &\sim N\left(0, \frac{2T^5}{15}\right)
                \end{align*}
            \end{proof}
    \end{enumerate}
    

\section{Reflection principle of the Wiener process}
    \begin{enumerate}[label=\roman*)]
        \item Show that $\Pr(M_T \ge a) = 2 \Pr(W(T) \ge a)$.
            \begin{proof}
                The core point in this proof is showing that the conditional
                probability for $\Pr(W(T) \ge a | W(\tau) = a, \tau \in [0,T])
                = 0.5$. This follows because the rest of the series $t \in
                [\tau, T]$ is
                distributed symmatrically about $W(\tau) = a$. Notice that if
                $W(\tau) = a$, we must have $M_T \ge
                a$. Thus we have by bayes theorem,
                \begin{align*}
                    \Pr(W(T) \ge a | M(T) \ge a) &= 0.5 = \frac{\Pr(M(T) \ge a |
                    W(T) \ge a)\Pr(W(T) \ge a)}{\Pr(M_T \ge a)}
                    \\
                    \Pr(M_T \ge a) &= 2\Pr(W(T) \ge a)
                \end{align*}
            \end{proof}
        \item Find the PDF of $M_T$. 
            \begin{proof}
                We look at the related probabilities in order to determine the
                pdf of $M_T$. We have, 
                \begin{align*}
                    \Pr(M_T \ge a) &= 2\Pr(W(T) \ge a)
                    \\
                    \int_a^{\infty} \rho_{M_T}(x)dx &= 2\int_a^{\infty}
                    \rho_{W(T)}(x)dx
                    \\
                    \int_{a + dx}^{\infty} \rho_{M_T}(x)dx - \int_a^{\infty}
                    \rho_{M_T}(x)dx &=2\int_{a + dx}^{\infty} \rho_{W(T)}(x)dx
                    -2\int_a^{\infty} \rho_{W(T)}(x)dx
                    \\
                    \int_a^{a + dx} \rho_{M_T}(x)dx &= 2\int_a^{a + dx} \rho_{W(T)}(x)dx
                    \\
                    \lim_{dx \to 0} \int_a^{a + dx} \rho_{M_T}(x)dx &= \lim_{dx
                    \to 0}2\int_a^{a + dx} \rho_{W(T)}(x)dx
                    \\
                    \rho_{M_T}(x) &= 2\rho_{W(T)}(x), \quad x\in[0,\infty)
                    \\
                    &= \sqrt{\frac{2}{\pi T}}e^{-x^2/2T}, \quad x \ge 0
                \end{align*}
            \end{proof}
    \end{enumerate}

\section{Lambda-chain rule for stochastic integrals}
    \begin{enumerate}[label=\roman*)]
        \item Stratonovich interpretation
            \begin{proof}
                \begin{align*}
                    \int_0^T \cos(W(t))dW(t) &= \sin\left(W(t)\right)\Big|_0^T -
                    \int_0^T 0 dt
                    \\
                    &= \sin(W(T))
                \end{align*}
            \end{proof}
        \item Ito Interpretation
            \begin{proof}
                \begin{align*}
                    \int_0^T e^t\sin(W(t))dW(t) &= -e^t\cos(W(t))\Big|_0^T -
                    \int_0^T -e^t\cos(W(t)) + \frac{1}{2}e^t\cos(W(t))dt
                    \\
                    &= -e^T\cos(W(T)) + 1 + \frac{1}{2}\int_0^Te^t\cos(W(t))dt
                \end{align*}
            \end{proof}
    \end{enumerate}

\section{SDE for American Stocks}
    \begin{enumerate}[label=\roman*)]
        \item 
            \begin{align*}
                w &= E(w + w_z dW + w_s ds + w_{zz}/2 dW^2 + O(ds^2))
                \\
                &= w + w_s ds + w_{zz}/2 ds + O(ds^2)
                \\
                w_s &= -\frac{1}{2}w_{zz}
            \end{align*}
        \item 
            \begin{align*}
                w_s &= -\frac{1}{2}w_{zz}
                \\
                w(0,s) &= 0
                \\
                w(x_c, s) &= 1
                \\
                w(z, T) &= \begin{cases} 0 & z < x_c\\ 1 & z \ge
                x_c\end{cases}
            \end{align*}
        \item 
            \begin{align*}
                w_{\tau} &= \frac{1}{2}w_{zz}
                \\ 
                w(0,\tau) &= 0
                \\
                w(x_c, \tau) &= 1
                \\
                w(z, 0) &= \begin{cases} 0 & z < x_c\\ 1 & z \ge
                x_c\end{cases}
            \end{align*}
    \end{enumerate}
\section{SDE for European Stocks}
    \begin{enumerate}[label=\roman*)]
        \item 
            \begin{align*}
                w &= E(w + w_z(b ds + \sqrt{a}dW) + w_s ds + w_{zz}(b^2ds^2+
                adW^2)/2 + O(ds^2))
                \\
                &= w + w_s ds + b w_z ds + w_{zz}ds/2 + O(ds^2)
                \\
                w_s &= -bw_z -\frac{a}{2}w_{zz}
            \end{align*}
        \item 
            \begin{align*}
                w_s &= -bw_z -\frac{a}{2}w_{zz}
                \\
                w(0,s) &= 0
                \\
                w(z, T) &= \begin{cases} 0 & z < x_c\\ 1 & z \ge
                x_c\end{cases}
            \end{align*}
        \item No, since the probability is dependent on $X$ at time $T$ we have
        no boundary condition at $z = x_c$. That is, since $X(t)$ can cross back
        over $x_c$ even if it reaches $x_c$ before $t = T$. 
    \end{enumerate}

\end{document}
